(base) C:\Users\kimgr\Documents\Python Scripts\project1final>python project1.py
---------------------
Running main function
---------------------



Starting Project 1: part a


Checking if my code works correctly:
My MSE is:  0.0020413214826653725
My R2 score is:  0.9728704511829854

Testing against sklearn OLS:
MSE after scaling: 0.0020
R2 score after scaling 0.9729

Calculating and printing the Confidence Intervals for Beta:
Lower bound:    Beta Values:    Upper bound:
0.11038698731706006 0.17746684404815277 0.24454670077924548
9.700618483759012 10.422600643236773 11.144582802714535
4.628185231439319 5.317507343648132 6.006829455856946
-45.58971573100263 -42.43198326959819 -39.274250808193756
-26.67106664041826 -24.07917425632426 -21.487281872230263
-15.087312151252231 -11.892509359679254 -8.697706568106277
51.69247051708899 58.39923669575137 65.10600287441375
58.95716775794346 64.28559121574978 69.6140146735561
26.841096500600962 32.03516490818954 37.229233315778124
-12.667817912283537 -5.595821811701846 1.4761742888798448
-34.426835783948704 -27.583827488555045 -20.740819193161386
-79.15188944630515 -73.5889200493875 -68.02595065246985
-23.368422648433064 -18.274389026243277 -13.18035540405349
-43.56992727407906 -38.09027061600905 -32.61061395793904
22.358806938431464 29.67114326414014 36.98347958984882
-1.6555043091003165 1.0317532958430777 3.719010900786472
24.42623277012348 26.945176925662206 29.464121081200933
11.69427334943185 14.049718623311028 16.405163897190207
-4.8618112013451595 -2.5509336042296056 -0.24005600711405162
16.92169745025395 19.364625187894337 21.807552925534722
-20.29340419410243 -17.443404672536612 -14.593405150970796



Starting Project 1: part b

Generating figure 2.11 from Hastie..

Computing bias and variance trade-off as a function of model complexity

Polynomial degree: 1
Error: 0.022818759886034358
Bias^2: 0.022722449894384246
Var: 9.63099916501125e-05
0.022818759886034358 >= 0.022722449894384246 + 9.63099916501125e-05 = 0.022818759886034358
Polynomial degree: 2
Error: 0.017128497214279555
Bias^2: 0.016995142348200657
Var: 0.00013335486607890312
0.017128497214279555 >= 0.016995142348200657 + 0.00013335486607890312 = 0.01712849721427956
Polynomial degree: 3
Error: 0.008433185456196122
Bias^2: 0.008295747765347816
Var: 0.0001374376908483074
0.008433185456196122 >= 0.008295747765347816 + 0.0001374376908483074 = 0.008433185456196124
Polynomial degree: 4
Error: 0.004472983546139685
Bias^2: 0.004347004937290785
Var: 0.000125978608848901
0.004472983546139685 >= 0.004347004937290785 + 0.000125978608848901 = 0.004472983546139685
Polynomial degree: 5
Error: 0.002173938709195349
Bias^2: 0.0020433516128460093
Var: 0.00013058709634933922
0.002173938709195349 >= 0.0020433516128460093 + 0.00013058709634933922 = 0.0021739387091953485
Polynomial degree: 6
Error: 0.001341304057194481
Bias^2: 0.0012366332762856057
Var: 0.00010467078090887508
0.001341304057194481 >= 0.0012366332762856057 + 0.00010467078090887508 = 0.001341304057194481
Polynomial degree: 7
Error: 0.0006406951941488243
Bias^2: 0.000560593325416086
Var: 8.010186873273835e-05
0.0006406951941488243 >= 0.000560593325416086 + 8.010186873273835e-05 = 0.0006406951941488243
Polynomial degree: 8
Error: 0.00041003568567768824
Bias^2: 0.0003056709521737344
Var: 0.00010436473350395387
0.00041003568567768824 >= 0.0003056709521737344 + 0.00010436473350395387 = 0.0004100356856776883
Polynomial degree: 9
Error: 0.0003613653910531774
Bias^2: 0.00031460113003517535
Var: 4.67642610180021e-05
0.0003613653910531774 >= 0.00031460113003517535 + 4.67642610180021e-05 = 0.00036136539105317746

Starting Project 1: part c


Starting Cross-validation

Doing cross-validation for polynomial:  5
Average MSE calculated from OLS cross-validation:  0.0020161838920394696

Starting Project 1: part d

Starting bootstrap for polynomial:  1
MSE values for Ridge regression with bootstrap
[0.02414527 0.02411548 0.02412396 0.02414588 0.02414936 0.0241096
 0.0271755  0.13037288 0.23072786 0.24087563]
Starting bootstrap for polynomial:  2
MSE values for Ridge regression with bootstrap
[0.01856479 0.01857811 0.01858347 0.01859757 0.01873952 0.02110049
 0.03066583 0.10539236 0.25117212 0.2723619 ]
Starting bootstrap for polynomial:  3
MSE values for Ridge regression with bootstrap
[0.00928843 0.00928111 0.00928056 0.00929503 0.01121205 0.01911525
 0.03135974 0.08703154 0.24040574 0.26916354]
Starting bootstrap for polynomial:  4
MSE values for Ridge regression with bootstrap
[0.00524906 0.00530121 0.00518455 0.0068514  0.00934902 0.01615931
 0.03210622 0.07407086 0.22088727 0.25395235]
Starting bootstrap for polynomial:  5
MSE values for Ridge regression with bootstrap
[0.00211205 0.00209271 0.00327647 0.0049537  0.00840931 0.01292541
 0.02744226 0.06895825 0.21465485 0.25186432]
Starting bootstrap for polynomial:  6
MSE values for Ridge regression with bootstrap
[0.00128161 0.00172625 0.00228247 0.00405797 0.00721595 0.01125853
 0.02820869 0.05988317 0.1900926  0.22831759]
Starting bootstrap for polynomial:  7
MSE values for Ridge regression with bootstrap
[0.00164685 0.00168346 0.00215235 0.00458235 0.00826472 0.012563
 0.03037207 0.07391361 0.21250917 0.25451521]
Starting bootstrap for polynomial:  8
MSE values for Ridge regression with bootstrap
[0.0011194  0.00146779 0.00197185 0.00358617 0.00631947 0.01063481
 0.02869074 0.06805516 0.19331881 0.23544741]
Starting bootstrap for polynomial:  9
MSE values for Ridge regression with bootstrap
[0.00082341 0.00127235 0.00210086 0.00372477 0.00672167 0.01225481
 0.02905602 0.07762694 0.21545788 0.26078989]
Doing cross-validation for polynomial:  5
LASSO regression
C:\Users\kimgr\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49340617880061366, tolerance: 0.006377735910100573
  model = cd_fast.enet_coordinate_descent(
C:\Users\kimgr\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015535159499561857, tolerance: 0.006377735910100573
  model = cd_fast.enet_coordinate_descent(

Summary of regression

C:\Users\kimgr\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.49340617880061366, tolerance: 0.006377735910100573
  model = cd_fast.enet_coordinate_descent(
C:\Users\kimgr\anaconda3\lib\site-packages\sklearn\linear_model\_coordinate_descent.py:529: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 0.015535159499561857, tolerance: 0.006377735910100573
  model = cd_fast.enet_coordinate_descent(
Minimum values for polynomial: 5 are:
Minimum MSE values for LASSO : [0.00775365] for lambda:  [0.0001]
Minimum MSE values for Ridge : [0.00207766] for lambda:  [0.0001]
Minimum MSE values for OLS :  0.0020413214826653725


###########################################################
###########################################################

Starting Project 1: part f and g

Checking if my code works correctly:
My MSE is:  0.0007561693842943243
My R2 score is:  0.492497941489761

Testing against sklearn OLS:
MSE after scaling: 0.0008
R2 score after scaling 0.4925

Calculating and printing the Confidence Intervals for Beta:
Lower bound:    Beta Values:    Upper bound:
0.9293366995814017 0.9456104105043004 0.961884121427199
-0.3381910131290974 -0.17715885794583613 -0.016126702762574857
-0.33819101435125265 -0.17715885891718833 -0.016126703483124005
1.242280303183826 1.9162560372048496 2.590231771225873
1.2422803031801357 1.9162560372007404 2.590231771221345
1.2422803033016236 1.9162560373378965 2.590231771374169
-6.402679155653674 -5.112843554966767 -3.823007954279859
-6.402679154883219 -5.112843554284104 -3.8230079536849884
-6.402679154883179 -5.112843554284064 -3.8230079536849484
-6.40267915595234 -5.112843555215427 -3.8230079544785145
4.109487188632322 5.249135535753711 6.388783882875099
4.109487188698237 5.249135535853152 6.388783883008067
4.109487188629789 5.249135535775868 6.388783882921947
4.109487188629625 5.2491355357757055 6.388783882921786
4.10948718869815 5.249135535853065 6.38878388300798
-2.23196710618565 -1.854162786489142 -1.4763584667926337
-2.231967106184131 -1.8541627864554249 -1.4763584667267189
-2.2319671061502877 -1.854162786455442 -1.4763584667605965
-2.2319671060215756 -1.8541627863401873 -1.4763584666587992
-2.2319671060554187 -1.8541627863401702 -1.4763584666249214
-2.2319671060221453 -1.8541627863428878 -1.4763584666636302



Starting Project 1: part b

Generating figure 2.11 from Hastie..

Computing bias and variance trade-off as a function of model complexity

Polynomial degree: 1
Error: 0.004666795266810403
Bias^2: 0.004656027626875344
Var: 1.0767639935058295e-05
0.004666795266810403 >= 0.004656027626875344 + 1.0767639935058295e-05 = 0.004666795266810402
Polynomial degree: 2
Error: 0.002391677844140506
Bias^2: 0.0023775439557107017
Var: 1.413388842980278e-05
0.002391677844140506 >= 0.0023775439557107017 + 1.413388842980278e-05 = 0.0023916778441405045
Polynomial degree: 3
Error: 0.002495184825978537
Bias^2: 0.00248507910596036
Var: 1.0105720018177625e-05
0.002495184825978537 >= 0.00248507910596036 + 1.0105720018177625e-05 = 0.002495184825978538
Polynomial degree: 4
Error: 0.0015829915614512077
Bias^2: 0.0015692849382743057
Var: 1.3706623176901401e-05
0.0015829915614512077 >= 0.0015692849382743057 + 1.3706623176901401e-05 = 0.001582991561451207
Polynomial degree: 5
Error: 0.001167090852413872
Bias^2: 0.0011504745634811084
Var: 1.661628893276347e-05
0.001167090852413872 >= 0.0011504745634811084 + 1.661628893276347e-05 = 0.0011670908524138719
Polynomial degree: 6
Error: 0.0003069845945172647
Bias^2: 0.0002985904695423926
Var: 8.394124974872493e-06
0.0003069845945172647 >= 0.0002985904695423926 + 8.394124974872493e-06 = 0.0003069845945172651
Polynomial degree: 7
Error: 0.00045681591235028176
Bias^2: 0.000449882926681164
Var: 6.9329856691185185e-06
0.00045681591235028176 >= 0.000449882926681164 + 6.9329856691185185e-06 = 0.0004568159123502825
Polynomial degree: 8
Error: 0.001083245638411643
Bias^2: 0.0010785643251313103
Var: 4.681313280333603e-06
0.001083245638411643 >= 0.0010785643251313103 + 4.681313280333603e-06 = 0.001083245638411644
Polynomial degree: 9
Error: 0.00032346999331741113
Bias^2: 0.000315274699288314
Var: 8.19529402909717e-06
0.00032346999331741113 >= 0.000315274699288314 + 8.19529402909717e-06 = 0.00032346999331741113

Starting Project 1: part c


Starting Cross-validation

Doing cross-validation for polynomial:  5
Average MSE calculated from OLS cross-validation:  0.1180882914133665

Starting Project 1: part d

Starting bootstrap for polynomial:  1
MSE values for Ridge regression with bootstrap
[0.00150306 0.00150165 0.00150037 0.00150026 0.00152681 0.00287354
 0.08052688 0.36188355 0.44369948 0.44976987]
Starting bootstrap for polynomial:  2
MSE values for Ridge regression with bootstrap
[0.00266833 0.0026312  0.00263993 0.00265972 0.00274691 0.00491112
 0.03883524 0.31005946 0.44630427 0.45806991]
Starting bootstrap for polynomial:  3
MSE values for Ridge regression with bootstrap
[0.00323926 0.0032331  0.00323705 0.003232   0.00308614 0.00221349
 0.01273964 0.25448462 0.44129957 0.4592381 ]
Starting bootstrap for polynomial:  4
MSE values for Ridge regression with bootstrap
[0.00102744 0.00102626 0.00104038 0.00109786 0.0012234  0.00177827
 0.01410554 0.22328211 0.43254396 0.45463843]
Starting bootstrap for polynomial:  5
MSE values for Ridge regression with bootstrap
[0.00049075 0.00047551 0.00051066 0.00061524 0.00082235 0.00091424
 0.0155453  0.21158972 0.43609551 0.46178351]
Starting bootstrap for polynomial:  6
MSE values for Ridge regression with bootstrap
[0.00053848 0.00064165 0.00071704 0.00082413 0.00098814 0.00118777
 0.00915588 0.17401793 0.41857283 0.44869852]
Starting bootstrap for polynomial:  7
MSE values for Ridge regression with bootstrap
[4.49059874e-04 4.73664835e-04 5.06586724e-04 5.83799189e-04
 7.54840739e-04 1.09246215e-03 9.78516761e-03 1.65474747e-01
 4.18856274e-01 4.52034713e-01]
Starting bootstrap for polynomial:  8
MSE values for Ridge regression with bootstrap
[2.51410651e-04 3.33810983e-04 3.44379039e-04 3.82797446e-04
 5.76296163e-04 1.03559854e-03 9.34041496e-03 1.53890675e-01
 4.14054569e-01 4.50307868e-01]
Starting bootstrap for polynomial:  9
MSE values for Ridge regression with bootstrap
[0.00063436 0.00070861 0.00075545 0.0007972  0.00095108 0.00152504
 0.00991138 0.14784974 0.41277434 0.45163472]
Doing cross-validation for polynomial:  5
LASSO regression

Summary of regression

Minimum values for polynomial: 5 are:
Minimum MSE values for LASSO : [0.00095964] for lambda:  [0.0001]
Minimum MSE values for Ridge : [0.00083987] for lambda:  [0.0001]
Minimum MSE values for OLS :  0.0007561693842943243

(base) C:\Users\kimgr\Documents\Python Scripts\project1final>










